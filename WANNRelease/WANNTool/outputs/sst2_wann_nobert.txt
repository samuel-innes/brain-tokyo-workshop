
/home/students/innes/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11050). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)
  return torch._C._cuda_getDeviceCount() > 0
size of model 514
cuda available? False
We are executing on: cpu
('Backpropagation optimization started',)
EPOCH 1:
  batch 64 loss: 0.4779372925404459 corr: 0.75244140625
  batch 128 loss: 0.3223981794435531 corr: 0.8701171875
  batch 192 loss: 0.2797334261704236 corr: 0.88330078125
  batch 256 loss: 0.25278737558983266 corr: 0.89599609375
  batch 320 loss: 0.2502685993677005 corr: 0.90283203125
  batch 384 loss: 0.24944161856546998 corr: 0.8994140625
  batch 448 loss: 0.23279598902445287 corr: 0.90673828125
  batch 512 loss: 0.21218872693134472 corr: 0.91845703125
  batch 576 loss: 0.20191147399600595 corr: 0.919921875
  batch 640 loss: 0.18655868974747136 corr: 0.9267578125
  batch 704 loss: 0.2055232239072211 corr: 0.92138671875
  batch 768 loss: 0.22056012647226453 corr: 0.91259765625
  batch 832 loss: 0.18411831837147474 corr: 0.93115234375
  batch 896 loss: 0.19082078308565542 corr: 0.927734375
  batch 960 loss: 0.17993890604702756 corr: 0.93115234375
  batch 1024 loss: 0.18991078750696033 corr: 0.92333984375
  batch 1088 loss: 0.191857484576758 corr: 0.9248046875
  batch 1152 loss: 0.18335053429473191 corr: 0.93310546875
  batch 1216 loss: 0.17422244895715266 corr: 0.9345703125
  batch 1280 loss: 0.17272416059859097 corr: 0.93701171875
  batch 1344 loss: 0.1768531421548687 corr: 0.93115234375
  batch 1408 loss: 0.15097069540934172 corr: 0.94970703125
  batch 1472 loss: 0.15838060877285898 corr: 0.939453125
  batch 1536 loss: 0.1497608560603112 corr: 0.9453125
  batch 1600 loss: 0.1624296373629477 corr: 0.93701171875
  batch 1664 loss: 0.1753557653282769 corr: 0.93701171875
  batch 1728 loss: 0.15831233214703389 corr: 0.94140625
  batch 1792 loss: 0.16077356808818877 corr: 0.94482421875
  batch 1856 loss: 0.16267476481152698 corr: 0.9384765625
  batch 1920 loss: 0.15643065291806124 corr: 0.94775390625
  batch 1984 loss: 0.1527256182162091 corr: 0.9384765625
  batch 2048 loss: 0.15421592950588092 corr: 0.94482421875
LOSS train 0.19695374338341226 valid 0.2080545425415039
ACCCURACY train 0.8953384798099763 valid 0.921875
EPOCH 2:
  batch 64 loss: 0.08695547345996602 corr: 0.97216796875
  batch 128 loss: 0.09324578347150236 corr: 0.96923828125
  batch 192 loss: 0.09834655567829031 corr: 0.9677734375
  batch 256 loss: 0.09203619875188451 corr: 0.96484375
  batch 320 loss: 0.08509007013344672 corr: 0.97119140625
  batch 384 loss: 0.11082535557216033 corr: 0.96435546875
  batch 448 loss: 0.103195130606764 corr: 0.96533203125
  batch 512 loss: 0.10979854142351542 corr: 0.9609375
  batch 576 loss: 0.0931024732417427 corr: 0.96826171875
  batch 640 loss: 0.08840797840093728 corr: 0.96826171875
  batch 704 loss: 0.10841836631880142 corr: 0.96533203125
  batch 768 loss: 0.13154590940393973 corr: 0.95703125
  batch 832 loss: 0.10425892040075269 corr: 0.9638671875
  batch 896 loss: 0.10047446502721868 corr: 0.966796875
  batch 960 loss: 0.0873181263305014 corr: 0.96875
  batch 1024 loss: 0.12541563640115783 corr: 0.95654296875
  batch 1088 loss: 0.11796734359813854 corr: 0.95849609375
  batch 1152 loss: 0.1241871386882849 corr: 0.95361328125
  batch 1216 loss: 0.10397608623316046 corr: 0.962890625
  batch 1280 loss: 0.09839280910091475 corr: 0.96533203125
  batch 1344 loss: 0.08842263341648504 corr: 0.9736328125
  batch 1408 loss: 0.1136675534362439 corr: 0.9619140625
  batch 1472 loss: 0.11392736577545293 corr: 0.962890625
  batch 1536 loss: 0.09997970874246676 corr: 0.9658203125
  batch 1600 loss: 0.11479663790669292 corr: 0.9599609375
  batch 1664 loss: 0.10288334258075338 corr: 0.962890625
  batch 1728 loss: 0.10360721009055851 corr: 0.962890625
  batch 1792 loss: 0.09380016077193432 corr: 0.97314453125
  batch 1856 loss: 0.10499689447897254 corr: 0.96337890625
  batch 1920 loss: 0.11218971095513552 corr: 0.9619140625
  batch 1984 loss: 0.13737546304764692 corr: 0.9560546875
  batch 2048 loss: 0.11886098474496976 corr: 0.95458984375
LOSS train 0.1023837652276414 valid 0.23950161039829254
ACCCURACY train 0.9379602137767221 valid 0.9285714285714286
EPOCH 3:
  batch 64 loss: 0.060947376798139885 corr: 0.97802734375
  batch 128 loss: 0.050045216226862976 corr: 0.9853515625
  batch 192 loss: 0.07179374327824917 corr: 0.9736328125
  batch 256 loss: 0.06146403706952697 corr: 0.9765625
  batch 320 loss: 0.0742396044661291 corr: 0.97802734375
  batch 384 loss: 0.05311156580137322 corr: 0.98388671875
  batch 448 loss: 0.07633910668664612 corr: 0.9755859375
  batch 512 loss: 0.07066426390520064 corr: 0.97509765625
  batch 576 loss: 0.05909330999566009 corr: 0.982421875
  batch 640 loss: 0.0597511799314816 corr: 0.97900390625
  batch 704 loss: 0.06088034706408507 corr: 0.97998046875
  batch 768 loss: 0.08255188555631321 corr: 0.9736328125
  batch 832 loss: 0.06574276987521444 corr: 0.97607421875
  batch 896 loss: 0.06524400701164268 corr: 0.97216796875
  batch 960 loss: 0.07578664375614608 corr: 0.974609375
  batch 1024 loss: 0.0622390822863963 corr: 0.9794921875
  batch 1088 loss: 0.08054013981382013 corr: 0.96923828125
  batch 1152 loss: 0.05939756926818518 corr: 0.9794921875
  batch 1216 loss: 0.06362616625119699 corr: 0.9794921875
  batch 1280 loss: 0.06844109786470653 corr: 0.97998046875
  batch 1344 loss: 0.0608925560991338 corr: 0.97900390625
  batch 1408 loss: 0.07709057313331868 corr: 0.97705078125
  batch 1472 loss: 0.07670849167334381 corr: 0.97509765625
  batch 1536 loss: 0.07273512323445175 corr: 0.97265625
  batch 1600 loss: 0.07371160948969191 corr: 0.97265625
  batch 1664 loss: 0.07491139648482203 corr: 0.97412109375
  batch 1728 loss: 0.08180765774886822 corr: 0.9736328125
  batch 1792 loss: 0.08981197343382519 corr: 0.9677734375
  batch 1856 loss: 0.06529724029678619 corr: 0.97509765625
  batch 1920 loss: 0.09554116599611007 corr: 0.96240234375
  batch 1984 loss: 0.08320007530710427 corr: 0.9638671875
  batch 2048 loss: 0.08002811409096466 corr: 0.96923828125
LOSS train 0.06851907161677216 valid 0.22781042754650116
ACCCURACY train 0.9490350356294537 valid 0.9185267857142857
EPOCH 4:
  batch 64 loss: 0.04300559431067086 corr: 0.986328125
  batch 128 loss: 0.043113204859764664 corr: 0.9853515625
  batch 192 loss: 0.04019754283945076 corr: 0.986328125
  batch 256 loss: 0.052269152703956934 corr: 0.9814453125
  batch 320 loss: 0.030791046139711398 corr: 0.98876953125
  batch 384 loss: 0.04845543994269974 corr: 0.98388671875
  batch 448 loss: 0.04099198814219562 corr: 0.98583984375
  batch 512 loss: 0.03939728627119621 corr: 0.98486328125
  batch 576 loss: 0.03827699511384708 corr: 0.9892578125
  batch 640 loss: 0.03596845295851381 corr: 0.98876953125
  batch 704 loss: 0.04823711671633646 corr: 0.98046875
  batch 768 loss: 0.045938103472508374 corr: 0.984375
  batch 832 loss: 0.0630879534091946 corr: 0.978515625
  batch 896 loss: 0.04220936924502894 corr: 0.98388671875
  batch 960 loss: 0.048263361903082114 corr: 0.9833984375
  batch 1024 loss: 0.049809869360615266 corr: 0.9833984375
  batch 1088 loss: 0.05613572535366984 corr: 0.98486328125
  batch 1152 loss: 0.048842288724699756 corr: 0.98291015625
  batch 1216 loss: 0.04421035301675147 corr: 0.98486328125
  batch 1280 loss: 0.05829155665287544 corr: 0.97998046875
  batch 1344 loss: 0.059386639353760984 corr: 0.97802734375
  batch 1408 loss: 0.05106650306152005 corr: 0.97998046875
  batch 1472 loss: 0.050966789429367054 corr: 0.98388671875
  batch 1536 loss: 0.05552703299326822 corr: 0.9833984375
  batch 1600 loss: 0.0437737876745814 corr: 0.9833984375
  batch 1664 loss: 0.055222688049980206 corr: 0.9814453125
  batch 1728 loss: 0.0525182551045873 corr: 0.98291015625
  batch 1792 loss: 0.06046854351370712 corr: 0.9765625
  batch 1856 loss: 0.04739045044516388 corr: 0.9814453125
  batch 1920 loss: 0.06357830839624512 corr: 0.97802734375
  batch 1984 loss: 0.06393259538890561 corr: 0.97412109375
  batch 2048 loss: 0.05768131855256797 corr: 0.97900390625
LOSS train 0.04800776248856397 valid 0.2490500509738922
ACCCURACY train 0.9561906175771971 valid 0.9229910714285714
EPOCH 5:
  batch 64 loss: 0.031717116116851685 corr: 0.99072265625
  batch 128 loss: 0.040325874149857555 corr: 0.98876953125
  batch 192 loss: 0.030564748214601423 corr: 0.9892578125
  batch 256 loss: 0.02452341212938336 corr: 0.99072265625
  batch 320 loss: 0.03334480895227898 corr: 0.98828125
  batch 384 loss: 0.038972803141405166 corr: 0.9853515625
  batch 448 loss: 0.024227730261372926 corr: 0.990234375
  batch 512 loss: 0.03222931926302408 corr: 0.98681640625
  batch 576 loss: 0.04057771953193878 corr: 0.98583984375
  batch 640 loss: 0.033212549997188034 corr: 0.98779296875
  batch 704 loss: 0.048460520651133265 corr: 0.984375
  batch 768 loss: 0.027343116969859693 corr: 0.990234375
  batch 832 loss: 0.03702376212186209 corr: 0.986328125
  batch 896 loss: 0.03889620530298998 corr: 0.986328125
  batch 960 loss: 0.041651990341051714 corr: 0.98583984375
  batch 1024 loss: 0.03865593155751412 corr: 0.98681640625
  batch 1088 loss: 0.04349753208953189 corr: 0.9833984375
  batch 1152 loss: 0.04147093703068094 corr: 0.984375
  batch 1216 loss: 0.03875968956708675 corr: 0.98681640625
  batch 1280 loss: 0.042325809433350514 corr: 0.9853515625
  batch 1344 loss: 0.04400471273766016 corr: 0.98828125
  batch 1408 loss: 0.0321792955469391 corr: 0.986328125
  batch 1472 loss: 0.03922792574758205 corr: 0.98583984375
  batch 1536 loss: 0.04117842142477457 corr: 0.9833984375
  batch 1600 loss: 0.02852679728584917 corr: 0.9892578125
  batch 1664 loss: 0.04396422995887406 corr: 0.9873046875
  batch 1728 loss: 0.04065474840535899 corr: 0.98388671875
  batch 1792 loss: 0.046606221499132516 corr: 0.98388671875
  batch 1856 loss: 0.0366787322836899 corr: 0.986328125
  batch 1920 loss: 0.05571803861312219 corr: 0.98193359375
  batch 1984 loss: 0.05149968489786261 corr: 0.982421875
  batch 2048 loss: 0.04283621818467509 corr: 0.982421875
LOSS train 0.037422718583440824 valid 0.2833048105239868
ACCCURACY train 0.9596941805225653 valid 0.9252232142857143
EPOCH 6:
  batch 64 loss: 0.01641578699445745 corr: 0.99267578125
  batch 128 loss: 0.022036880844552797 corr: 0.9921875
  batch 192 loss: 0.020002118179945683 corr: 0.9921875
  batch 256 loss: 0.032025788782902964 corr: 0.9892578125
  batch 320 loss: 0.02387244590136106 corr: 0.99169921875
  batch 384 loss: 0.028308894591646094 corr: 0.98974609375
  batch 448 loss: 0.027221475302667386 corr: 0.98828125
  batch 512 loss: 0.027057344360400748 corr: 0.9912109375
  batch 576 loss: 0.030124498659915844 corr: 0.99072265625
  batch 640 loss: 0.02201924021846935 corr: 0.99169921875
  batch 704 loss: 0.020575345213956098 corr: 0.9921875
  batch 768 loss: 0.021941034274277627 corr: 0.990234375
  batch 832 loss: 0.028332182043413923 corr: 0.98779296875
  batch 896 loss: 0.020915899950750827 corr: 0.994140625
  batch 960 loss: 0.030385542281692324 corr: 0.98828125
  batch 1024 loss: 0.028326902130174858 corr: 0.98779296875
  batch 1088 loss: 0.03366519823930503 corr: 0.98486328125
  batch 1152 loss: 0.03815626992582111 corr: 0.9873046875
  batch 1216 loss: 0.026925073762413376 corr: 0.990234375
  batch 1280 loss: 0.0340446872173743 corr: 0.98974609375
  batch 1344 loss: 0.024748627166445658 corr: 0.99072265625
  batch 1408 loss: 0.04503222920902772 corr: 0.98486328125
  batch 1472 loss: 0.020214263013713207 corr: 0.99365234375
  batch 1536 loss: 0.03603390026182751 corr: 0.9873046875
  batch 1600 loss: 0.030910092199519568 corr: 0.9873046875
  batch 1664 loss: 0.023476080820955758 corr: 0.98779296875
  batch 1728 loss: 0.024411062722265342 corr: 0.99072265625
  batch 1792 loss: 0.04192593671268696 corr: 0.98046875
  batch 1856 loss: 0.0408845749534521 corr: 0.98193359375
  batch 1920 loss: 0.04028259716596949 corr: 0.98388671875
  batch 1984 loss: 0.024838446648573154 corr: 0.990234375
  batch 2048 loss: 0.03548363023674028 corr: 0.9873046875
LOSS train 0.02798955781432173 valid 0.2781979441642761
ACCCURACY train 0.9622327790973871 valid 0.921875
EPOCH 7:
  batch 64 loss: 0.01923759546389192 corr: 0.9921875
  batch 128 loss: 0.012153860343005363 corr: 0.99560546875
  batch 192 loss: 0.014366664311864952 corr: 0.99365234375
  batch 256 loss: 0.014681942674314996 corr: 0.9931640625
  batch 320 loss: 0.029528581791282704 corr: 0.9912109375
  batch 384 loss: 0.01930843313311925 corr: 0.99267578125
  batch 448 loss: 0.023441360115612042 corr: 0.990234375
  batch 512 loss: 0.01652492371090375 corr: 0.99267578125
  batch 576 loss: 0.020970320769720274 corr: 0.99365234375
  batch 640 loss: 0.023536692259767733 corr: 0.9912109375
  batch 704 loss: 0.02266163089393558 corr: 0.9931640625
  batch 768 loss: 0.01633272664184915 corr: 0.99462890625
  batch 832 loss: 0.018070058362127384 corr: 0.9931640625
  batch 896 loss: 0.019407066672329165 corr: 0.994140625
  batch 960 loss: 0.03294502141295652 corr: 0.98876953125
  batch 1024 loss: 0.024669356645517837 corr: 0.9921875
  batch 1088 loss: 0.02618529487062915 corr: 0.99072265625
  batch 1152 loss: 0.02358958886475193 corr: 0.98974609375
  batch 1216 loss: 0.020865723435463224 corr: 0.99365234375
  batch 1280 loss: 0.023865437506401577 corr: 0.99267578125
  batch 1344 loss: 0.02051764983070825 corr: 0.98974609375
  batch 1408 loss: 0.025212882888126842 corr: 0.99072265625
  batch 1472 loss: 0.028579087898492617 corr: 0.98828125
  batch 1536 loss: 0.015888915812411142 corr: 0.99462890625
  batch 1600 loss: 0.04129454125313714 corr: 0.98583984375
  batch 1664 loss: 0.03194829422136536 corr: 0.98828125
  batch 1728 loss: 0.03849343320780463 corr: 0.9873046875
  batch 1792 loss: 0.020266179578584342 corr: 0.990234375
  batch 1856 loss: 0.02221435943056349 corr: 0.98876953125
  batch 1920 loss: 0.022643343052095588 corr: 0.99072265625
  batch 1984 loss: 0.024920378798924503 corr: 0.990234375
  batch 2048 loss: 0.016617433888086452 corr: 0.9931640625
LOSS train 0.02222331681869058 valid 0.30022791028022766
ACCCURACY train 0.9646229216152019 valid 0.9073660714285714
EPOCH 8:
  batch 64 loss: 0.009593995878958594 corr: 0.99658203125
  batch 128 loss: 0.01134413857414529 corr: 0.99755859375
  batch 192 loss: 0.013502768790203845 corr: 0.9970703125
  batch 256 loss: 0.02158660161080661 corr: 0.9912109375
  batch 320 loss: 0.01358056716298961 corr: 0.99365234375
  batch 384 loss: 0.015916522687575707 corr: 0.994140625
  batch 448 loss: 0.013252529107376176 corr: 0.9951171875
  batch 512 loss: 0.022396107554413902 corr: 0.9912109375
  batch 576 loss: 0.017965172909953253 corr: 0.994140625
  batch 640 loss: 0.016657303038073223 corr: 0.994140625
  batch 704 loss: 0.028145330025608928 corr: 0.9921875
  batch 768 loss: 0.017519469519811537 corr: 0.99365234375
  batch 832 loss: 0.013971161121844489 corr: 0.994140625
  batch 896 loss: 0.022143358270341196 corr: 0.99169921875
  batch 960 loss: 0.02297387168437126 corr: 0.99072265625
  batch 1024 loss: 0.030746864461434598 corr: 0.990234375
  batch 1088 loss: 0.01844028789969343 corr: 0.9921875
  batch 1152 loss: 0.020534716388510788 corr: 0.99267578125
  batch 1216 loss: 0.01820207689615927 corr: 0.99365234375
  batch 1280 loss: 0.023021138759759197 corr: 0.99267578125
  batch 1344 loss: 0.01936888105842627 corr: 0.990234375
  batch 1408 loss: 0.019641473238607432 corr: 0.9931640625
  batch 1472 loss: 0.012022113417629043 corr: 0.9951171875
  batch 1536 loss: 0.022105901558688856 corr: 0.9921875
  batch 1600 loss: 0.021944846580709054 corr: 0.98974609375
  batch 1664 loss: 0.023532311186727384 corr: 0.990234375
  batch 1728 loss: 0.012710893062262585 corr: 0.9951171875
  batch 1792 loss: 0.0269168599444356 corr: 0.9892578125
  batch 1856 loss: 0.03063225117375623 corr: 0.9892578125
  batch 1920 loss: 0.020168825876680785 corr: 0.9912109375
  batch 1984 loss: 0.019290171840793846 corr: 0.99365234375
  batch 2048 loss: 0.032317053940914775 corr: 0.986328125
LOSS train 0.019219627636193073 valid 0.3691411018371582
ACCCURACY train 0.965751187648456 valid 0.9107142857142857
EPOCH 9:
  batch 64 loss: 0.010670866255168221 corr: 0.99560546875
  batch 128 loss: 0.014367950186056078 corr: 0.9921875
  batch 192 loss: 0.011443316884424348 corr: 0.99609375
  batch 256 loss: 0.0179559555817832 corr: 0.9931640625
  batch 320 loss: 0.016777620424932138 corr: 0.9951171875
  batch 384 loss: 0.018946882761838424 corr: 0.994140625
  batch 448 loss: 0.013097969440764246 corr: 0.994140625
  batch 512 loss: 0.02138500239948371 corr: 0.99365234375
  batch 576 loss: 0.02112814838937993 corr: 0.9912109375
  batch 640 loss: 0.018458976723650267 corr: 0.9921875
  batch 704 loss: 0.025515930533742903 corr: 0.99267578125
  batch 768 loss: 0.014782998605824105 corr: 0.99267578125
  batch 832 loss: 0.01468030477440152 corr: 0.9951171875
  batch 896 loss: 0.016084730635725464 corr: 0.9951171875
  batch 960 loss: 0.026355435415098327 corr: 0.99072265625
  batch 1024 loss: 0.02547189015058393 corr: 0.99072265625
  batch 1088 loss: 0.011235946341912495 corr: 0.99462890625
  batch 1152 loss: 0.013183423069335731 corr: 0.99462890625
  batch 1216 loss: 0.02100133385238223 corr: 0.99072265625
  batch 1280 loss: 0.022456555746657614 corr: 0.9921875
  batch 1344 loss: 0.01561753868395499 corr: 0.99365234375
  batch 1408 loss: 0.02324599444409614 corr: 0.98876953125
  batch 1472 loss: 0.02452350501835099 corr: 0.99072265625
  batch 1536 loss: 0.025522510057726322 corr: 0.99169921875
  batch 1600 loss: 0.019958495962328016 corr: 0.9951171875
  batch 1664 loss: 0.010443657560244901 corr: 0.99755859375
  batch 1728 loss: 0.024329029263981283 corr: 0.99267578125
  batch 1792 loss: 0.016121467964239855 corr: 0.99267578125
  batch 1856 loss: 0.01977064797915773 corr: 0.99169921875
  batch 1920 loss: 0.026549828121687824 corr: 0.990234375
  batch 1984 loss: 0.018797292485032813 corr: 0.99365234375
  batch 2048 loss: 0.021810392386214517 corr: 0.99267578125
LOSS train 0.01829371129615689 valid 0.3729933202266693
ACCCURACY train 0.966166864608076 valid 0.9073660714285714
EPOCH 10:
  batch 64 loss: 0.012110921276871522 corr: 0.99609375
  batch 128 loss: 0.011212396177597839 corr: 0.9951171875
  batch 192 loss: 0.014561516822254816 corr: 0.99560546875
  batch 256 loss: 0.015782036925429566 corr: 0.99267578125
  batch 320 loss: 0.010872350722365809 corr: 0.99658203125
  batch 384 loss: 0.009401576758136798 corr: 0.99560546875
  batch 448 loss: 0.01859173891989485 corr: 0.99072265625
  batch 512 loss: 0.00932676809992472 corr: 0.9970703125
  batch 576 loss: 0.008852796927953932 corr: 0.998046875
  batch 640 loss: 0.014145070222127742 corr: 0.99267578125
  batch 704 loss: 0.013892121502294685 corr: 0.99658203125
  batch 768 loss: 0.017408586207807275 corr: 0.9951171875
  batch 832 loss: 0.01788544535793335 corr: 0.9931640625
  batch 896 loss: 0.012216632786930859 corr: 0.99609375
  batch 960 loss: 0.008136647604771952 corr: 0.99658203125
  batch 1024 loss: 0.013462398407966703 corr: 0.9951171875
  batch 1088 loss: 0.016472691708258935 corr: 0.9921875
  batch 1152 loss: 0.005931601377824336 corr: 0.998046875
  batch 1216 loss: 0.021348403585079723 corr: 0.99365234375
  batch 1280 loss: 0.008730253482099215 corr: 0.99658203125
  batch 1344 loss: 0.025386414948343372 corr: 0.99267578125
  batch 1408 loss: 0.02288186305668205 corr: 0.9912109375
  batch 1472 loss: 0.02622350913588889 corr: 0.9912109375
  batch 1536 loss: 0.01914042902217261 corr: 0.9921875
  batch 1600 loss: 0.018735961771881193 corr: 0.99267578125
  batch 1664 loss: 0.022005934125445492 corr: 0.9921875
  batch 1728 loss: 0.01614685227514201 corr: 0.9951171875
  batch 1792 loss: 0.026724504668436566 corr: 0.99169921875
  batch 1856 loss: 0.021355069669425575 corr: 0.9912109375
  batch 1920 loss: 0.012332945702610232 corr: 0.994140625
  batch 1984 loss: 0.02552343753882269 corr: 0.990234375
  batch 2048 loss: 0.02227493510417844 corr: 0.9912109375
LOSS train 0.015781816608609712 valid 0.3612043261528015
ACCCURACY train 0.9671169833729216 valid 0.9162946428571429
