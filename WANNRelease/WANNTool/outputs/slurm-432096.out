size of model 514
We are executing on: cuda:0
('Backpropagation optimization started',)
EPOCH 1:
  batch 64 loss: 0.5470698052085936 corr: 0.24832185638099669
  batch 128 loss: 0.46114966087043285 corr: 0.4754895812873783
  batch 192 loss: 0.4362144179176539 corr: 0.5065214594541647
  batch 256 loss: 0.4429675079882145 corr: 0.4977793321536918
LOSS train 0.45072272047400475 valid 0.40877828001976013
MATTHEW CORRELATION train 0.41268351743910003 valid 0.5592810292861147
EPOCH 2:
  batch 64 loss: 0.27334184967912734 corr: 0.7357091976235405
  batch 128 loss: 0.28248989896383137 corr: 0.7150438480807856
  batch 192 loss: 0.26693197118584067 corr: 0.7350681787861496
  batch 256 loss: 0.27849810768384486 corr: 0.7281024174253086
LOSS train 0.26298789910749715 valid 0.48470833897590637
MATTHEW CORRELATION train 0.6958623622485455 valid 0.534596389025463
EPOCH 3:
  batch 64 loss: 0.15198043387499638 corr: 0.8863281135610684
  batch 128 loss: 0.13666861853562295 corr: 0.886687954319297
  batch 192 loss: 0.17390240848180838 corr: 0.8342751001791329
  batch 256 loss: 0.16530056641204283 corr: 0.8382332008280313
LOSS train 0.14993481249061982 valid 0.43730539083480835
MATTHEW CORRELATION train 0.8228117895850817 valid 0.5919913034826743
EPOCH 4:
  batch 64 loss: 0.09440632432233542 corr: 0.9176393400601229
  batch 128 loss: 0.09367125125572784 corr: 0.9288549563624072
  batch 192 loss: 0.10859709726355504 corr: 0.9120158391659707
  batch 256 loss: 0.11320405335573014 corr: 0.8950188282691135
LOSS train 0.09788148685309814 valid 0.6342423558235168
MATTHEW CORRELATION train 0.8724845286824153 valid 0.538674657306291
EPOCH 5:
  batch 64 loss: 0.05972010098048486 corr: 0.9477889949447851
  batch 128 loss: 0.06769702216843143 corr: 0.936656022264213
  batch 192 loss: 0.06724849151214585 corr: 0.9441147988840097
  batch 256 loss: 0.07959566496720072 corr: 0.935440629941418
LOSS train 0.06549523095600307 valid 0.729318380355835
MATTHEW CORRELATION train 0.8988657781574748 valid 0.5587520296374445
EPOCH 6:
  batch 64 loss: 0.04571386331372196 corr: 0.9707909043391849
  batch 128 loss: 0.057962279290222796 corr: 0.944401185377989
  batch 192 loss: 0.06153108726721257 corr: 0.9519709068529356
  batch 256 loss: 0.05346062339958735 corr: 0.959071089193213
LOSS train 0.052219188840774845 valid 0.8720743656158447
MATTHEW CORRELATION train 0.9137275428688532 valid 0.5471174830675325
EPOCH 7:
  batch 64 loss: 0.03427801694670052 corr: 0.9710427368220083
  batch 128 loss: 0.037432041001011385 corr: 0.9640809999459077
  batch 192 loss: 0.047354413272842066 corr: 0.9576442973256656
  batch 256 loss: 0.04158090009696025 corr: 0.9626753913322873
LOSS train 0.03836307374746608 valid 0.9442816376686096
MATTHEW CORRELATION train 0.9207029075643867 valid 0.5498464414633656
EPOCH 8:
  batch 64 loss: 0.02737626971429563 corr: 0.9693981976782564
  batch 128 loss: 0.04048302681349014 corr: 0.9665623919485792
  batch 192 loss: 0.03363708476717875 corr: 0.9659510543156323
  batch 256 loss: 0.04292382061430544 corr: 0.966559878748035
LOSS train 0.03448840642609432 valid 0.9663239121437073
MATTHEW CORRELATION train 0.9238140949708663 valid 0.5355345547986046
EPOCH 9:
  batch 64 loss: 0.021636666324411635 corr: 0.9822068832653956
  batch 128 loss: 0.02544401705290511 corr: 0.9751596088523364
  batch 192 loss: 0.020984714044971042 corr: 0.9836163504708508
  batch 256 loss: 0.032923471480899025 corr: 0.9768206345599382
LOSS train 0.02411674481270133 valid 0.7456212639808655
MATTHEW CORRELATION train 0.9355948602145722 valid 0.5669730047553136
EPOCH 10:
  batch 64 loss: 0.03063148913133773 corr: 0.9766880962920422
  batch 128 loss: 0.020423265260433254 corr: 0.9818994967294943
  batch 192 loss: 0.029236130827030138 corr: 0.9798345478876874
  batch 256 loss: 0.035482429439070984 corr: 0.974171344356317
LOSS train 0.027647358724267965 valid 0.7684745788574219
MATTHEW CORRELATION train 0.9343506830484873 valid 0.5602063994306369
